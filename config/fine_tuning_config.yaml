fine_tuning:
  base_model: "mistralai/Mistral-7B-Instruct-v0.2"
  dataset_path: "./data/complete_dataset.json"
  
  # LoRA Configuration
  lora:
    r: 16
    lora_alpha: 32
    lora_dropout: 0.05
    target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  
  # Training Configuration - ALL VALUES AS FLOATS/INTS
  training:
    output_dir: "./models/formatter_fine_tuned"
    num_train_epochs: 3
    per_device_train_batch_size: 1
    per_device_eval_batch_size: 1
    gradient_accumulation_steps: 4
    learning_rate: 0.0002  # Changed from 2e-4 to explicit float
    warmup_steps: 100
    logging_steps: 10
    save_steps: 200
    eval_steps: 200
    max_seq_length: 1024
    
  # Evaluation
  evaluation:
    eval_dataset_size: 0.1
    early_stopping_patience: 3