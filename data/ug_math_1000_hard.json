[
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^6 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=5/9$ with $n=6$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=5/9$ and $n=6$ to evaluate numerically: $S_n\\approx 4.516740$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=121 modulo m=150, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}1&1\\\\3&-1\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 87/209 conversions and variant B has 69/528. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=-0.2856$; SE=0.0371. 95% CI=(-0.3583,-0.2128). Conclusion: B is significantly worse at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^3$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[-1, -3, 1]$ and $v=[-4, 1, -1]$ one has $\\langle u,v\\rangle=0$, $\\|u\\|=3.3166$, $\\|v\\|=4.2426$, and indeed $|0|\\le 14.0712$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^8$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[2, 2, 3, -3, -4, 2, 2, 4]$ and $v=[1, -2, 3, -2, -1, -1, -1, -4]$ one has $\\langle u,v\\rangle=-3$, $\\|u\\|=8.1240$, $\\|v\\|=6.0828$, and indeed $|-3|\\le 49.4166$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=18 modulo m=190, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=132 modulo m=156, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 114/821 conversions and variant B has 100/214. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=0.3284$; SE=0.0362. 95% CI=(0.2575,0.3993). Conclusion: B is significantly better at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{1x}\\cos(5x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{1x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(4)$ and $Y\\sim\\mathrm{Exp}(3)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{3-4}(e^{-4s}-e^{-3s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=80 modulo m=109, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 5-unit and 9-unit coins to make exactly 62 units. Find all nonnegative integer solutions (x,y) to 5x+9y=62, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 5x+9y=62 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+9t$, $y=y_0-5t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{3x}\\cos(4x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{3x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{1x}\\cos(5x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{1x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[8]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^8-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{5^2}+\\frac{y^2}{6^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=43 modulo m=135, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.01. A test has sensitivity 0.95 and specificity 0.96. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.1935. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9995. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (-7)x^2 + (15)x + (-10)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=2$ is a root. Perform synthetic division to factor $(x-2)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 4, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (-5)x^2 + (5)x + (-1)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=1$ is a root. Perform synthetic division to factor $(x-1)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[9]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^9-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 5, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (0)x^2 + (-2)x + (-1)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=-1$ is a root. Perform synthetic division to factor $(x--1)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 374/860 conversions and variant B has 103/451. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=-0.2065$; SE=0.0260. 95% CI=(-0.2575,-0.1555). Conclusion: B is significantly worse at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 393/791 conversions and variant B has 225/484. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=-0.0320$; SE=0.0288. 95% CI=(-0.0884,0.0245). Conclusion: B is not significantly different at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(5)$ and $Y\\sim\\mathrm{Exp}(3)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{3-5}(e^{-5s}-e^{-3s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 3-unit and 5-unit coins to make exactly 43 units. Find all nonnegative integer solutions (x,y) to 3x+5y=43, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 3x+5y=43 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+5t$, $y=y_0-3t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(4)$ and $Y\\sim\\mathrm{Exp}(2)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{2-4}(e^{-4s}-e^{-2s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^7$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[4, -2, 4, -2, 3, 3, 2]$ and $v=[-4, 0, 3, 1, 2, -1, 4]$ one has $\\langle u,v\\rangle=5$, $\\|u\\|=7.8740$, $\\|v\\|=6.8557$, and indeed $|5|\\le 53.9815$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}2&-1\\\\-4&-3\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{5^2}+\\frac{y^2}{4^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.01. A test has sensitivity 0.92 and specificity 0.96. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.1885. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9992. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 7, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{1x}\\cos(3x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{1x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[6]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^6-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=80 modulo m=99, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=52, mean1=0.37, sd1=3.07) and (n2=31, mean2=0.24, sd2=1.93). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 7.3138$. Test statistic $t=\\dfrac{0.37-0.24}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx 0.2118$ with df=81. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{1x}\\cos(2x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{1x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^11 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=8/9$ with $n=11$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=8/9$ and $n=11$ to evaluate numerically: $S_n\\approx 31.728616$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^7 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=8/5$ with $n=7$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=8/5$ and $n=7$ to evaluate numerically: $S_n\\approx 241.387072$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=23 modulo m=84, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^6$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[4, 4, 1, 1, -4, -2]$ and $v=[-2, 1, 4, 1, -1, -2]$ one has $\\langle u,v\\rangle=9$, $\\|u\\|=7.3485$, $\\|v\\|=5.1962$, and indeed $|9|\\le 38.1838$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{3^2}+\\frac{y^2}{3^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (-3)x^2 + (7)x + (-10)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=2$ is a root. Perform synthetic division to factor $(x-2)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 5-unit and 9-unit coins to make exactly 110 units. Find all nonnegative integer solutions (x,y) to 5x+9y=110, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 5x+9y=110 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+9t$, $y=y_0-5t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[9]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^9-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{4^2}+\\frac{y^2}{5^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}2&1\\\\3&-3\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=24, mean1=5.84, sd1=1.29) and (n2=18, mean2=0.79, sd2=3.16). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 5.2007$. Test statistic $t=\\dfrac{5.84-0.79}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx 7.1019$ with df=40. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[7]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^7-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=35 modulo m=185, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{3^2}+\\frac{y^2}{5^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 71/264 conversions and variant B has 113/592. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=-0.0781$; SE=0.0317. 95% CI=(-0.1402,-0.0159). Conclusion: B is significantly worse at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(5)$ and $Y\\sim\\mathrm{Exp}(1)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{1-5}(e^{-5s}-e^{-1s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.05. A test has sensitivity 0.95 and specificity 0.93. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.4167. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9972. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=26, mean1=3.72, sd1=1.82) and (n2=21, mean2=9.68, sd2=3.09). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 6.0838$. Test statistic $t=\\dfrac{3.72-9.68}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx -8.2358$ with df=45. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[3]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^3-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=19, mean1=4.81, sd1=3.13) and (n2=40, mean2=2.2, sd2=2.92). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 8.9276$. Test statistic $t=\\dfrac{4.81-2.2}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx 3.1351$ with df=57. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=41 modulo m=156, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}-2&-2\\\\0&-1\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{2x}\\cos(3x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{2x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^7$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[-4, -2, -3, 3, 0, -2, 1]$ and $v=[1, 4, 2, -3, 3, 2, 1]$ one has $\\langle u,v\\rangle=-30$, $\\|u\\|=6.5574$, $\\|v\\|=6.6332$, and indeed $|-30|\\le 43.4971$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{1x}\\cos(3x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{1x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[4]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^4-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(3)$ and $Y\\sim\\mathrm{Exp}(2)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{2-3}(e^{-3s}-e^{-2s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 261/621 conversions and variant B has 217/928. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=-0.1865$; SE=0.0242. 95% CI=(-0.2339,-0.1390). Conclusion: B is significantly worse at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^4$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[-3, 0, 0, -2]$ and $v=[2, 1, 2, -4]$ one has $\\langle u,v\\rangle=2$, $\\|u\\|=3.6056$, $\\|v\\|=5.0000$, and indeed $|2|\\le 18.0278$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=50, mean1=1.45, sd1=1.17) and (n2=37, mean2=4.09, sd2=1.12). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 1.3204$. Test statistic $t=\\dfrac{1.45-4.09}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx -10.5944$ with df=85. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{2x}\\cos(3x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{2x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[7]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^7-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{3^2}+\\frac{y^2}{3^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}4&0\\\\2&-2\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(3)$ and $Y\\sim\\mathrm{Exp}(2)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{2-3}(e^{-3s}-e^{-2s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.02. A test has sensitivity 0.92 and specificity 0.90. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.1581. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9982. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=30, mean1=3.75, sd1=3.55) and (n2=45, mean2=0.73, sd2=1.88). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 7.1368$. Test statistic $t=\\dfrac{3.75-0.73}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx 4.7961$ with df=73. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{2x}\\cos(5x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{2x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}0&0\\\\-3&3\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 216/609 conversions and variant B has 94/475. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=-0.1568$; SE=0.0266. 95% CI=(-0.2090,-0.1046). Conclusion: B is significantly worse at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^3$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[-4, -1, 2]$ and $v=[0, -2, 0]$ one has $\\langle u,v\\rangle=2$, $\\|u\\|=4.5826$, $\\|v\\|=2.0000$, and indeed $|2|\\le 9.1652$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(5)$ and $Y\\sim\\mathrm{Exp}(4)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{4-5}(e^{-5s}-e^{-4s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{4^2}+\\frac{y^2}{3^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[3]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^3-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (1)x^2 + (-4)x + (2)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=1$ is a root. Perform synthetic division to factor $(x-1)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=40, mean1=6.71, sd1=4.0) and (n2=38, mean2=5.92, sd2=2.19). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 10.5455$. Test statistic $t=\\dfrac{6.71-5.92}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx 1.0739$ with df=76. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=121 modulo m=144, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[4]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^4-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^12 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=7/8$ with $n=12$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=7/8$ and $n=12$ to evaluate numerically: $S_n\\approx 31.773242$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 5, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[6]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^6-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 225/451 conversions and variant B has 53/531. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=-0.3991$; SE=0.0269. 95% CI=(-0.4518,-0.3464). Conclusion: B is significantly worse at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 99/347 conversions and variant B has 64/345. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=-0.0998$; SE=0.0320. 95% CI=(-0.1626,-0.0370). Conclusion: B is significantly worse at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{3x}\\cos(2x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{3x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}-4&-3\\\\0&4\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.01. A test has sensitivity 0.95 and specificity 0.93. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.1206. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9995. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{1x}\\cos(3x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{1x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.02. A test has sensitivity 0.92 and specificity 0.96. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.3194. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9983. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{3^2}+\\frac{y^2}{4^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 150/435 conversions and variant B has 131/936. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=-0.2049$; SE=0.0255. 95% CI=(-0.2548,-0.1550). Conclusion: B is significantly worse at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^3$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[4, 0, 2]$ and $v=[4, 1, 1]$ one has $\\langle u,v\\rangle=18$, $\\|u\\|=4.4721$, $\\|v\\|=4.2426$, and indeed $|18|\\le 18.9737$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{5^2}+\\frac{y^2}{3^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[5]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^5-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=25, mean1=2.8, sd1=2.43) and (n2=49, mean2=3.58, sd2=3.95). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 12.3700$. Test statistic $t=\\dfrac{2.8-3.58}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx -0.9023$ with df=72. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 86/262 conversions and variant B has 228/993. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=-0.0986$; SE=0.0319. 95% CI=(-0.1612,-0.0360). Conclusion: B is significantly worse at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.10. A test has sensitivity 0.95 and specificity 0.93. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.6013. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9941. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 7, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=46 modulo m=161, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(5)$ and $Y\\sim\\mathrm{Exp}(4)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{4-5}(e^{-5s}-e^{-4s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=56, mean1=6.34, sd1=3.91) and (n2=44, mean2=9.71, sd2=2.26). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 10.8211$. Test statistic $t=\\dfrac{6.34-9.71}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx -5.0853$ with df=98. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{3x}\\cos(2x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{3x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (-4)x^2 + (7)x + (-12)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=3$ is a root. Perform synthetic division to factor $(x-3)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (-6)x^2 + (3)x + (2)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=1$ is a root. Perform synthetic division to factor $(x-1)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=56, mean1=5.73, sd1=2.42) and (n2=23, mean2=9.97, sd2=3.08). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 6.8935$. Test statistic $t=\\dfrac{5.73-9.97}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx -6.5206$ with df=77. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^5$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[2, 1, -4, -2, 2]$ and $v=[-3, -2, 1, -1, -3]$ one has $\\langle u,v\\rangle=-16$, $\\|u\\|=5.3852$, $\\|v\\|=4.8990$, and indeed $|-16|\\le 26.3818$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{4^2}+\\frac{y^2}{4^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 327/855 conversions and variant B has 404/935. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=0.0496$; SE=0.0232. 95% CI=(0.0041,0.0951). Conclusion: B is significantly better at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (0)x^2 + (-9)x + (0)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=3$ is a root. Perform synthetic division to factor $(x-3)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 8, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 5-unit and 9-unit coins to make exactly 92 units. Find all nonnegative integer solutions (x,y) to 5x+9y=92, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 5x+9y=92 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+9t$, $y=y_0-5t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^9 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=8/5$ with $n=9$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=8/5$ and $n=9$ to evaluate numerically: $S_n\\approx 842.682493$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[6]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^6-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=80 modulo m=118, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=60 modulo m=91, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (1)x^2 + (-11)x + (10)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=2$ is a root. Perform synthetic division to factor $(x-2)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 7, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 7, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.02. A test has sensitivity 0.97 and specificity 0.93. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.2205. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9993. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{5^2}+\\frac{y^2}{6^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 8, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{3x}\\cos(2x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{3x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.01. A test has sensitivity 0.95 and specificity 0.96. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.1935. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9995. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^6$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[-3, -2, -4, 1, 3, -1]$ and $v=[-1, -1, 2, 1, -2, -3]$ one has $\\langle u,v\\rangle=-5$, $\\|u\\|=6.3246$, $\\|v\\|=4.4721$, and indeed $|-5|\\le 28.2843$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}2&2\\\\4&-3\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.05. A test has sensitivity 0.95 and specificity 0.90. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.3333. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9971. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=46, mean1=4.27, sd1=3.16) and (n2=29, mean2=7.73, sd2=2.31). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 8.2022$. Test statistic $t=\\dfrac{4.27-7.73}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx -5.0952$ with df=73. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.05. A test has sensitivity 0.97 and specificity 0.90. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.3380. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9982. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=14 modulo m=184, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 3-unit and 5-unit coins to make exactly 46 units. Find all nonnegative integer solutions (x,y) to 3x+5y=46, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 3x+5y=46 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+5t$, $y=y_0-3t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=36, mean1=7.49, sd1=3.6) and (n2=25, mean2=2.69, sd2=1.58). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 8.7036$. Test statistic $t=\\dfrac{7.49-2.69}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx 6.2495$ with df=59. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 154/873 conversions and variant B has 193/505. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=0.2058$; SE=0.0252. 95% CI=(0.1564,0.2551). Conclusion: B is significantly better at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{3^2}+\\frac{y^2}{6^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 5-unit and 9-unit coins to make exactly 108 units. Find all nonnegative integer solutions (x,y) to 5x+9y=108, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 5x+9y=108 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+9t$, $y=y_0-5t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^4$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[-4, -1, 0, -1]$ and $v=[-3, 1, -3, 1]$ one has $\\langle u,v\\rangle=10$, $\\|u\\|=4.2426$, $\\|v\\|=4.4721$, and indeed $|10|\\le 18.9737$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 353/781 conversions and variant B has 305/752. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=-0.0464$; SE=0.0253. 95% CI=(-0.0959,0.0031). Conclusion: B is not significantly different at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^4$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[2, -3, 3, -4]$ and $v=[0, -2, 3, -3]$ one has $\\langle u,v\\rangle=27$, $\\|u\\|=6.1644$, $\\|v\\|=4.6904$, and indeed $|27|\\le 28.9137$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 6, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=17, mean1=3.75, sd1=3.48) and (n2=42, mean2=7.78, sd2=3.64). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 12.9298$. Test statistic $t=\\dfrac{3.75-7.78}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx -3.8988$ with df=57. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[2]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^2-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=20, mean1=3.22, sd1=1.79) and (n2=34, mean2=5.74, sd2=3.93). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 10.9723$. Test statistic $t=\\dfrac{3.22-5.74}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx -2.6997$ with df=52. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{3x}\\cos(3x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{3x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^11 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=6/8$ with $n=11$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=6/8$ and $n=11$ to evaluate numerically: $S_n\\approx 13.465892$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.01. A test has sensitivity 0.95 and specificity 0.93. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.1206. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9995. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^9 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=6/5$ with $n=9$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=6/5$ and $n=9$ to evaluate numerically: $S_n\\approx 128.195607$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[2]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^2-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}2&3\\\\-1&-1\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (0)x^2 + (-18)x + (8)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=4$ is a root. Perform synthetic division to factor $(x-4)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(1)$ and $Y\\sim\\mathrm{Exp}(4)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{4-1}(e^{-1s}-e^{-4s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.01. A test has sensitivity 0.97 and specificity 0.90. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.0892. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9997. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^3$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[0, -3, 2]$ and $v=[0, -3, -4]$ one has $\\langle u,v\\rangle=1$, $\\|u\\|=3.6056$, $\\|v\\|=5.0000$, and indeed $|1|\\le 18.0278$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(2)$ and $Y\\sim\\mathrm{Exp}(5)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{5-2}(e^{-2s}-e^{-5s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^12 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=3/2$ with $n=12$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=3/2$ and $n=12$ to evaluate numerically: $S_n\\approx 2598.926758$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(4)$ and $Y\\sim\\mathrm{Exp}(5)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{5-4}(e^{-4s}-e^{-5s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=39 modulo m=100, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 5-unit and 9-unit coins to make exactly 111 units. Find all nonnegative integer solutions (x,y) to 5x+9y=111, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 5x+9y=111 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+9t$, $y=y_0-5t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{4^2}+\\frac{y^2}{5^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 3-unit and 5-unit coins to make exactly 77 units. Find all nonnegative integer solutions (x,y) to 3x+5y=77, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 3x+5y=77 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+5t$, $y=y_0-3t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^6$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[1, -3, 0, -3, 0, 4]$ and $v=[1, -4, 2, -4, 2, 0]$ one has $\\langle u,v\\rangle=25$, $\\|u\\|=5.9161$, $\\|v\\|=6.4031$, and indeed $|25|\\le 37.8814$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{5^2}+\\frac{y^2}{4^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^6 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=6/4$ with $n=6$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=6/4$ and $n=6$ to evaluate numerically: $S_n\\approx 95.125000$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=30, mean1=4.78, sd1=2.45) and (n2=25, mean2=5.91, sd2=1.79). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 4.7353$. Test statistic $t=\\dfrac{4.78-5.91}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx -1.9176$ with df=53. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.01. A test has sensitivity 0.92 and specificity 0.96. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.1885. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9992. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{3x}\\cos(5x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{3x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{5^2}+\\frac{y^2}{3^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^5 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=6/7$ with $n=5$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=6/7$ and $n=5$ to evaluate numerically: $S_n\\approx 10.136193$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 5-unit and 9-unit coins to make exactly 120 units. Find all nonnegative integer solutions (x,y) to 5x+9y=120, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 5x+9y=120 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+9t$, $y=y_0-5t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{1x}\\cos(3x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{1x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{2x}\\cos(2x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{2x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 223/622 conversions and variant B has 116/462. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=-0.1074$; SE=0.0279. 95% CI=(-0.1621,-0.0528). Conclusion: B is significantly worse at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 92/312 conversions and variant B has 242/635. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=0.0862$; SE=0.0322. 95% CI=(0.0231,0.1494). Conclusion: B is significantly better at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}2&-3\\\\-2&-3\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{5^2}+\\frac{y^2}{5^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 5-unit and 9-unit coins to make exactly 92 units. Find all nonnegative integer solutions (x,y) to 5x+9y=92, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 5x+9y=92 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+9t$, $y=y_0-5t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=16, mean1=4.73, sd1=2.89) and (n2=15, mean2=3.25, sd2=2.75). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 7.9709$. Test statistic $t=\\dfrac{4.73-3.25}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx 1.4586$ with df=29. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[8]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^8-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 171/793 conversions and variant B has 214/978. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=0.0032$; SE=0.0197. 95% CI=(-0.0354,0.0418). Conclusion: B is not significantly different at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.02. A test has sensitivity 0.97 and specificity 0.90. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.1652. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9993. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 4, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}4&-4\\\\4&-1\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^5 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=2/4$ with $n=5$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=2/4$ and $n=5$ to evaluate numerically: $S_n\\approx 3.562500$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=33, mean1=8.39, sd1=1.85) and (n2=22, mean2=6.11, sd2=1.42). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 2.8654$. Test statistic $t=\\dfrac{8.39-6.11}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx 4.8936$ with df=53. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (8)x^2 + (18)x + (9)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=-3$ is a root. Perform synthetic division to factor $(x--3)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{5^2}+\\frac{y^2}{6^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}1&-2\\\\-2&-2\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^9 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=7/4$ with $n=9$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=7/4$ and $n=9$ to evaluate numerically: $S_n\\approx 1575.353897$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}-2&2\\\\-4&1\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{2x}\\cos(5x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{2x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=34 modulo m=138, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 7, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 5-unit and 9-unit coins to make exactly 66 units. Find all nonnegative integer solutions (x,y) to 5x+9y=66, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 5x+9y=66 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+9t$, $y=y_0-5t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^6$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[-4, -2, 3, -4, 1, 3]$ and $v=[3, 0, 2, -4, -2, 4]$ one has $\\langle u,v\\rangle=20$, $\\|u\\|=7.4162$, $\\|v\\|=7.0000$, and indeed $|20|\\le 51.9134$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{5^2}+\\frac{y^2}{6^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{4^2}+\\frac{y^2}{3^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^10 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=4/5$ with $n=10$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=4/5$ and $n=10$ to evaluate numerically: $S_n\\approx 16.946936$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}-1&-2\\\\-3&0\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 260/936 conversions and variant B has 155/849. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=-0.0952$; SE=0.0198. 95% CI=(-0.1339,-0.0565). Conclusion: B is significantly worse at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 4, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.01. A test has sensitivity 0.92 and specificity 0.90. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.0850. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9991. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.02. A test has sensitivity 0.92 and specificity 0.96. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.3194. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9983. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=36, mean1=4.63, sd1=3.77) and (n2=40, mean2=4.36, sd2=3.37). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 12.7077$. Test statistic $t=\\dfrac{4.63-4.36}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx 0.3297$ with df=74. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(2)$ and $Y\\sim\\mathrm{Exp}(5)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{5-2}(e^{-2s}-e^{-5s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.05. A test has sensitivity 0.95 and specificity 0.93. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.4167. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9972. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=12 modulo m=133, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{3x}\\cos(2x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{3x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 4-unit and 7-unit coins to make exactly 77 units. Find all nonnegative integer solutions (x,y) to 4x+7y=77, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 4x+7y=77 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+7t$, $y=y_0-4t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 8, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[6]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^6-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[6]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^6-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}3&2\\\\-1&4\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 7, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^6 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=3/6$ with $n=6$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=3/6$ and $n=6$ to evaluate numerically: $S_n\\approx 3.750000$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^11 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=8/9$ with $n=11$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=8/9$ and $n=11$ to evaluate numerically: $S_n\\approx 31.728616$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[9]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^9-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=33 modulo m=75, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.05. A test has sensitivity 0.97 and specificity 0.96. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.5607. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9984. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}1&-4\\\\-1&-1\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 5-unit and 9-unit coins to make exactly 87 units. Find all nonnegative integer solutions (x,y) to 5x+9y=87, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 5x+9y=87 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+9t$, $y=y_0-5t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[6]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^6-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (-1)x^2 + (-8)x + (12)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=-3$ is a root. Perform synthetic division to factor $(x--3)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(5)$ and $Y\\sim\\mathrm{Exp}(3)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{3-5}(e^{-5s}-e^{-3s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}-1&-3\\\\2&-3\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{1x}\\cos(5x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{1x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 5-unit and 9-unit coins to make exactly 93 units. Find all nonnegative integer solutions (x,y) to 5x+9y=93, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 5x+9y=93 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+9t$, $y=y_0-5t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.10. A test has sensitivity 0.92 and specificity 0.90. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.5055. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9902. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(1)$ and $Y\\sim\\mathrm{Exp}(4)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{4-1}(e^{-1s}-e^{-4s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{5^2}+\\frac{y^2}{4^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{5^2}+\\frac{y^2}{4^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 3-unit and 5-unit coins to make exactly 62 units. Find all nonnegative integer solutions (x,y) to 3x+5y=62, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 3x+5y=62 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+5t$, $y=y_0-3t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 256/627 conversions and variant B has 162/896. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=-0.2275$; SE=0.0235. 95% CI=(-0.2735,-0.1815). Conclusion: B is significantly worse at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^4$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[3, 2, -1, 2]$ and $v=[-4, -4, -2, -3]$ one has $\\langle u,v\\rangle=-24$, $\\|u\\|=4.2426$, $\\|v\\|=6.7082$, and indeed $|-24|\\le 28.4605$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (0)x^2 + (-12)x + (-9)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=-3$ is a root. Perform synthetic division to factor $(x--3)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}-2&-2\\\\3&3\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.05. A test has sensitivity 0.97 and specificity 0.96. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.5607. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9984. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (-3)x^2 + (-2)x + (2)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=-1$ is a root. Perform synthetic division to factor $(x--1)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[7]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^7-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=12 modulo m=64, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=158 modulo m=194, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{3x}\\cos(2x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{3x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 376/831 conversions and variant B has 89/323. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=-0.1769$; SE=0.0303. 95% CI=(-0.2362,-0.1176). Conclusion: B is significantly worse at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.05. A test has sensitivity 0.97 and specificity 0.90. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.3380. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9982. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(3)$ and $Y\\sim\\mathrm{Exp}(5)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{5-3}(e^{-3s}-e^{-5s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 276/869 conversions and variant B has 37/342. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=-0.2094$; SE=0.0231. 95% CI=(-0.2546,-0.1642). Conclusion: B is significantly worse at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[5]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^5-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(1)$ and $Y\\sim\\mathrm{Exp}(3)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{3-1}(e^{-1s}-e^{-3s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 8, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=35, mean1=3.19, sd1=3.28) and (n2=49, mean2=2.75, sd2=2.14). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 7.1415$. Test statistic $t=\\dfrac{3.19-2.75}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx 0.7440$ with df=82. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=24 modulo m=60, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 8, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 170/880 conversions and variant B has 273/607. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=0.2566$; SE=0.0242. 95% CI=(0.2092,0.3040). Conclusion: B is significantly better at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.01. A test has sensitivity 0.97 and specificity 0.96. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.1968. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9997. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=49, mean1=7.87, sd1=2.84) and (n2=38, mean2=3.73, sd2=3.65). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 10.3539$. Test statistic $t=\\dfrac{7.87-3.73}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx 5.9522$ with df=85. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(1)$ and $Y\\sim\\mathrm{Exp}(4)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{4-1}(e^{-1s}-e^{-4s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 104/746 conversions and variant B has 139/570. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=0.1044$; SE=0.0220. 95% CI=(0.0613,0.1476). Conclusion: B is significantly better at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^11 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=8/3$ with $n=11$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=8/3$ and $n=11$ to evaluate numerically: $S_n\\approx 302580.656895$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (-5)x^2 + (4)x + (6)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=3$ is a root. Perform synthetic division to factor $(x-3)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{3x}\\cos(3x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{3x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^5$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[4, -3, -3, 3, 4]$ and $v=[4, 0, -1, 4, -3]$ one has $\\langle u,v\\rangle=19$, $\\|u\\|=7.6811$, $\\|v\\|=6.4807$, and indeed $|19|\\le 49.7795$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.10. A test has sensitivity 0.97 and specificity 0.93. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.6063. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9964. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{3^2}+\\frac{y^2}{6^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.10. A test has sensitivity 0.95 and specificity 0.90. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.5135. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9939. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 5-unit and 9-unit coins to make exactly 63 units. Find all nonnegative integer solutions (x,y) to 5x+9y=63, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 5x+9y=63 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+9t$, $y=y_0-5t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.02. A test has sensitivity 0.92 and specificity 0.96. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.3194. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9983. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (-1)x^2 + (0)x + (0)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=1$ is a root. Perform synthetic division to factor $(x-1)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[9]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^9-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(1)$ and $Y\\sim\\mathrm{Exp}(2)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{2-1}(e^{-1s}-e^{-2s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(2)$ and $Y\\sim\\mathrm{Exp}(1)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{1-2}(e^{-2s}-e^{-1s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}0&-3\\\\-1&-1\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 5, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 8, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 219/882 conversions and variant B has 372/940. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=0.1474$; SE=0.0216. 95% CI=(0.1051,0.1898). Conclusion: B is significantly better at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^6$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[-4, 4, 2, -3, 4, 2]$ and $v=[-4, 1, 0, 3, -4, 4]$ one has $\\langle u,v\\rangle=3$, $\\|u\\|=8.0623$, $\\|v\\|=7.6158$, and indeed $|3|\\le 61.4003$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 6, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=27, mean1=8.44, sd1=3.76) and (n2=49, mean2=1.46, sd2=1.16). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 5.8401$. Test statistic $t=\\dfrac{8.44-1.46}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx 12.0509$ with df=74. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.02. A test has sensitivity 0.97 and specificity 0.96. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.3311. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9994. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 8, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{2x}\\cos(4x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{2x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^8$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[2, 1, 1, -1, 3, 1, -1, -2]$ and $v=[2, -4, 2, 2, 4, 0, -2, 4]$ one has $\\langle u,v\\rangle=6$, $\\|u\\|=4.6904$, $\\|v\\|=8.0000$, and indeed $|6|\\le 37.5233$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 3-unit and 5-unit coins to make exactly 50 units. Find all nonnegative integer solutions (x,y) to 3x+5y=50, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 3x+5y=50 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+5t$, $y=y_0-3t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 121/684 conversions and variant B has 215/798. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=0.0925$; SE=0.0214. 95% CI=(0.0505,0.1345). Conclusion: B is significantly better at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(1)$ and $Y\\sim\\mathrm{Exp}(3)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{3-1}(e^{-1s}-e^{-3s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=77 modulo m=98, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 8, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(3)$ and $Y\\sim\\mathrm{Exp}(4)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{4-3}(e^{-3s}-e^{-4s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{3x}\\cos(5x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{3x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}4&2\\\\3&-4\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 5-unit and 9-unit coins to make exactly 83 units. Find all nonnegative integer solutions (x,y) to 5x+9y=83, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 5x+9y=83 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+9t$, $y=y_0-5t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 67/250 conversions and variant B has 274/935. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=0.0250$; SE=0.0317. 95% CI=(-0.0371,0.0872). Conclusion: B is not significantly different at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 6, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^3$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[2, -3, 2]$ and $v=[-3, -4, 1]$ one has $\\langle u,v\\rangle=8$, $\\|u\\|=4.1231$, $\\|v\\|=5.0990$, and indeed $|8|\\le 21.0238$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.05. A test has sensitivity 0.97 and specificity 0.90. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.3380. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9982. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=19, mean1=4.97, sd1=1.32) and (n2=33, mean2=8.35, sd2=3.01). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 6.4257$. Test statistic $t=\\dfrac{4.97-8.35}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx -4.6301$ with df=50. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 50/288 conversions and variant B has 423/856. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=0.3205$; SE=0.0281. 95% CI=(0.2655,0.3756). Conclusion: B is significantly better at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{1x}\\cos(4x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{1x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(3)$ and $Y\\sim\\mathrm{Exp}(5)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{5-3}(e^{-3s}-e^{-5s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^10 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=5/9$ with $n=10$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=5/9$ and $n=10$ to evaluate numerically: $S_n\\approx 4.985304$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}-2&3\\\\4&-1\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=58, mean1=8.14, sd1=3.38) and (n2=35, mean2=7.78, sd2=3.13). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 10.8163$. Test statistic $t=\\dfrac{8.14-7.78}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx 0.5114$ with df=91. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.10. A test has sensitivity 0.97 and specificity 0.96. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.7293. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9965. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}-4&0\\\\1&1\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 5, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(4)$ and $Y\\sim\\mathrm{Exp}(3)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{3-4}(e^{-4s}-e^{-3s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}2&-2\\\\4&2\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 8, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(2)$ and $Y\\sim\\mathrm{Exp}(5)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{5-2}(e^{-2s}-e^{-5s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 5-unit and 9-unit coins to make exactly 90 units. Find all nonnegative integer solutions (x,y) to 5x+9y=90, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 5x+9y=90 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+9t$, $y=y_0-5t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.02. A test has sensitivity 0.92 and specificity 0.93. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.2115. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9982. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 5-unit and 9-unit coins to make exactly 110 units. Find all nonnegative integer solutions (x,y) to 5x+9y=110, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 5x+9y=110 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+9t$, $y=y_0-5t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[9]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^9-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.02. A test has sensitivity 0.97 and specificity 0.90. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.1652. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9993. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^11 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=8/6$ with $n=11$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=8/6$ and $n=11$ to evaluate numerically: $S_n\\approx 577.247252$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=158 modulo m=163, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}2&-2\\\\1&-1\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 3-unit and 5-unit coins to make exactly 70 units. Find all nonnegative integer solutions (x,y) to 3x+5y=70, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 3x+5y=70 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+5t$, $y=y_0-3t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 5, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=33, mean1=8.06, sd1=2.0) and (n2=46, mean2=5.67, sd2=1.21). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 2.5180$. Test statistic $t=\\dfrac{8.06-5.67}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx 6.6023$ with df=77. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 5-unit and 9-unit coins to make exactly 89 units. Find all nonnegative integer solutions (x,y) to 5x+9y=89, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 5x+9y=89 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+9t$, $y=y_0-5t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[5]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^5-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=47, mean1=2.46, sd1=3.61) and (n2=25, mean2=8.59, sd2=1.42). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 9.2553$. Test statistic $t=\\dfrac{2.46-8.59}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx -8.1399$ with df=70. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 5-unit and 9-unit coins to make exactly 92 units. Find all nonnegative integer solutions (x,y) to 5x+9y=92, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 5x+9y=92 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+9t$, $y=y_0-5t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=23 modulo m=156, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^8$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[3, -3, -4, -3, -2, -2, -4, 0]$ and $v=[2, 3, 0, 3, -4, 0, 1, 4]$ one has $\\langle u,v\\rangle=-8$, $\\|u\\|=8.1854$, $\\|v\\|=7.4162$, and indeed $|-8|\\le 60.7042$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (1)x^2 + (-7)x + (20)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=-4$ is a root. Perform synthetic division to factor $(x--4)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}4&4\\\\2&-1\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^7$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[1, -3, 0, 4, -1, -1, 1]$ and $v=[0, 2, -4, 4, -3, -1, 4]$ one has $\\langle u,v\\rangle=18$, $\\|u\\|=5.3852$, $\\|v\\|=7.8740$, and indeed $|18|\\le 42.4028$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}-1&-2\\\\-1&-4\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 3-unit and 5-unit coins to make exactly 63 units. Find all nonnegative integer solutions (x,y) to 3x+5y=63, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 3x+5y=63 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+5t$, $y=y_0-3t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 8, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^10 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=8/7$ with $n=10$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=8/7$ and $n=10$ to evaluate numerically: $S_n\\approx 128.824970$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 193/738 conversions and variant B has 105/809. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=-0.1317$; SE=0.0200. 95% CI=(-0.1710,-0.0925). Conclusion: B is significantly worse at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (7)x^2 + (10)x + (-8)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=-4$ is a root. Perform synthetic division to factor $(x--4)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{4^2}+\\frac{y^2}{5^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=44, mean1=4.96, sd1=2.38) and (n2=48, mean2=8.92, sd2=2.82). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 6.8592$. Test statistic $t=\\dfrac{4.96-8.92}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx -7.2445$ with df=90. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[2]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^2-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (3)x^2 + (-3)x + (-10)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=-2$ is a root. Perform synthetic division to factor $(x--2)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(4)$ and $Y\\sim\\mathrm{Exp}(1)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{1-4}(e^{-4s}-e^{-1s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[3]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^3-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (-1)x^2 + (-7)x + (3)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=3$ is a root. Perform synthetic division to factor $(x-3)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 3-unit and 5-unit coins to make exactly 77 units. Find all nonnegative integer solutions (x,y) to 3x+5y=77, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 3x+5y=77 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+5t$, $y=y_0-3t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{1x}\\cos(5x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{1x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}3&-3\\\\1&-3\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[7]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^7-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{2x}\\cos(3x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{2x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^5$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[4, 4, 4, -2, 1]$ and $v=[4, -3, 4, 1, -1]$ one has $\\langle u,v\\rangle=17$, $\\|u\\|=7.2801$, $\\|v\\|=6.5574$, and indeed $|17|\\le 47.7389$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 4-unit and 7-unit coins to make exactly 51 units. Find all nonnegative integer solutions (x,y) to 4x+7y=51, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 4x+7y=51 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+7t$, $y=y_0-4t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{4^2}+\\frac{y^2}{3^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{3^2}+\\frac{y^2}{4^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^4$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[-3, 3, 1, 4]$ and $v=[-4, 3, 4, 1]$ one has $\\langle u,v\\rangle=29$, $\\|u\\|=5.9161$, $\\|v\\|=6.4807$, and indeed $|29|\\le 38.3406$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 211/638 conversions and variant B has 75/424. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=-0.1538$; SE=0.0263. 95% CI=(-0.2053,-0.1023). Conclusion: B is significantly worse at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^11 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=6/9$ with $n=11$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=6/9$ and $n=11$ to evaluate numerically: $S_n\\approx 8.514437$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 234/743 conversions and variant B has 160/763. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=-0.1052$; SE=0.0225. 95% CI=(-0.1494,-0.0611). Conclusion: B is significantly worse at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(5)$ and $Y\\sim\\mathrm{Exp}(4)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{4-5}(e^{-5s}-e^{-4s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{2x}\\cos(2x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{2x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[4]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^4-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(4)$ and $Y\\sim\\mathrm{Exp}(2)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{2-4}(e^{-4s}-e^{-2s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(3)$ and $Y\\sim\\mathrm{Exp}(2)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{2-3}(e^{-3s}-e^{-2s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (-1)x^2 + (-12)x + (0)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=-3$ is a root. Perform synthetic division to factor $(x--3)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (0)x^2 + (-4)x + (3)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=1$ is a root. Perform synthetic division to factor $(x-1)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^7$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[-4, 3, -2, -1, -4, 0, 4]$ and $v=[-2, 4, -1, 0, 3, 3, -1]$ one has $\\langle u,v\\rangle=6$, $\\|u\\|=7.8740$, $\\|v\\|=6.3246$, and indeed $|6|\\le 49.7996$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=41, mean1=0.21, sd1=3.6) and (n2=60, mean2=1.74, sd2=1.03). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 5.8686$. Test statistic $t=\\dfrac{0.21-1.74}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx -3.1170$ with df=99. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=24 modulo m=124, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}-4&-3\\\\0&-3\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 8, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.01. A test has sensitivity 0.92 and specificity 0.90. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.0850. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9991. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=33, mean1=3.18, sd1=3.1) and (n2=29, mean2=1.34, sd2=3.82). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 11.9351$. Test statistic $t=\\dfrac{3.18-1.34}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx 2.0925$ with df=60. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(5)$ and $Y\\sim\\mathrm{Exp}(3)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{3-5}(e^{-5s}-e^{-3s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{4^2}+\\frac{y^2}{6^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (3)x^2 + (-3)x + (-10)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=-2$ is a root. Perform synthetic division to factor $(x--2)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 4-unit and 7-unit coins to make exactly 81 units. Find all nonnegative integer solutions (x,y) to 4x+7y=81, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 4x+7y=81 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+7t$, $y=y_0-4t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=8 modulo m=164, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{2x}\\cos(3x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{2x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 162/485 conversions and variant B has 129/430. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=-0.0340$; SE=0.0308. 95% CI=(-0.0943,0.0263). Conclusion: B is not significantly different at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^6 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=4/6$ with $n=6$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=4/6$ and $n=6$ to evaluate numerically: $S_n\\approx 6.629630$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{2^2}+\\frac{y^2}{4^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 3-unit and 5-unit coins to make exactly 77 units. Find all nonnegative integer solutions (x,y) to 3x+5y=77, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 3x+5y=77 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+5t$, $y=y_0-3t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[5]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^5-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{3x}\\cos(2x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{3x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 4-unit and 7-unit coins to make exactly 71 units. Find all nonnegative integer solutions (x,y) to 4x+7y=71, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 4x+7y=71 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+7t$, $y=y_0-4t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^10 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=4/3$ with $n=10$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=4/3$ and $n=10$ to evaluate numerically: $S_n\\approx 381.912259$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (-5)x^2 + (-1)x + (5)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=1$ is a root. Perform synthetic division to factor $(x-1)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}-3&-1\\\\4&0\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^6$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[3, -4, -4, -2, 3, -1]$ and $v=[-4, 0, 1, 0, 2, 2]$ one has $\\langle u,v\\rangle=-12$, $\\|u\\|=7.4162$, $\\|v\\|=5.0000$, and indeed $|-12|\\le 37.0810$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}3&1\\\\1&-1\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{4^2}+\\frac{y^2}{6^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.01. A test has sensitivity 0.95 and specificity 0.90. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.0876. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9994. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 6, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^7 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=4/6$ with $n=7$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=4/6$ and $n=7$ to evaluate numerically: $S_n\\approx 7.244170$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=27, mean1=1.59, sd1=3.21) and (n2=51, mean2=6.23, sd2=1.56). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 5.1261$. Test statistic $t=\\dfrac{1.59-6.23}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx -8.6108$ with df=76. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=45, mean1=5.99, sd1=3.06) and (n2=59, mean2=5.32, sd2=3.2). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 9.8619$. Test statistic $t=\\dfrac{5.99-5.32}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx 1.0780$ with df=102. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 3-unit and 5-unit coins to make exactly 75 units. Find all nonnegative integer solutions (x,y) to 3x+5y=75, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 3x+5y=75 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+5t$, $y=y_0-3t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[7]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^7-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[5]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^5-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 4-unit and 7-unit coins to make exactly 82 units. Find all nonnegative integer solutions (x,y) to 4x+7y=82, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 4x+7y=82 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+7t$, $y=y_0-4t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 371/911 conversions and variant B has 254/886. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=-0.1206$; SE=0.0223. 95% CI=(-0.1642,-0.0769). Conclusion: B is significantly worse at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (1)x^2 + (-3)x + (-3)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=-1$ is a root. Perform synthetic division to factor $(x--1)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^8$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[-4, 0, 1, 2, 1, -1, -4, -4]$ and $v=[4, -3, -3, -1, 0, -1, -2, 4]$ one has $\\langle u,v\\rangle=-28$, $\\|u\\|=7.4162$, $\\|v\\|=7.4833$, and indeed $|-28|\\le 55.4977$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^6$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[-1, 4, 1, -4, 2, 4]$ and $v=[2, 4, 3, 4, 2, 1]$ one has $\\langle u,v\\rangle=9$, $\\|u\\|=7.3485$, $\\|v\\|=7.0711$, and indeed $|9|\\le 51.9615$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(5)$ and $Y\\sim\\mathrm{Exp}(1)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{1-5}(e^{-5s}-e^{-1s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 5, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.05. A test has sensitivity 0.95 and specificity 0.90. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.3333. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9971. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{4^2}+\\frac{y^2}{4^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 238/933 conversions and variant B has 80/751. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=-0.1486$; SE=0.0182. 95% CI=(-0.1842,-0.1129). Conclusion: B is significantly worse at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{4^2}+\\frac{y^2}{4^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 7, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^10 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=4/9$ with $n=10$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=4/9$ and $n=10$ to evaluate numerically: $S_n\\approx 3.233613$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{4^2}+\\frac{y^2}{5^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[5]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^5-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{1x}\\cos(5x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{1x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 5-unit and 9-unit coins to make exactly 80 units. Find all nonnegative integer solutions (x,y) to 5x+9y=80, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 5x+9y=80 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+9t$, $y=y_0-5t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^8$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[-4, 4, 2, 4, 0, -2, 0, 2]$ and $v=[1, -2, 4, 1, 4, -3, -2, 2]$ one has $\\langle u,v\\rangle=10$, $\\|u\\|=7.7460$, $\\|v\\|=7.4162$, and indeed $|10|\\le 57.4456$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[8]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^8-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 5, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^12 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=5/9$ with $n=12$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=5/9$ and $n=12$ to evaluate numerically: $S_n\\approx 5.034784$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=12 modulo m=105, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 7, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (-7)x^2 + (16)x + (-12)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=3$ is a root. Perform synthetic division to factor $(x-3)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[7]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^7-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=120 modulo m=129, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=146 modulo m=147, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[4]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^4-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=40, mean1=5.48, sd1=2.38) and (n2=19, mean2=3.81, sd2=3.65). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 8.0827$. Test statistic $t=\\dfrac{5.48-3.81}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx 2.1082$ with df=57. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 185/583 conversions and variant B has 53/422. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=-0.1917$; SE=0.0251. 95% CI=(-0.2410,-0.1425). Conclusion: B is significantly worse at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^3$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[0, 1, -1]$ and $v=[4, -4, 1]$ one has $\\langle u,v\\rangle=-5$, $\\|u\\|=1.4142$, $\\|v\\|=5.7446$, and indeed $|-5|\\le 8.1240$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (9)x^2 + (25)x + (20)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=-4$ is a root. Perform synthetic division to factor $(x--4)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (-6)x^2 + (5)x + (12)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=3$ is a root. Perform synthetic division to factor $(x-3)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=73 modulo m=74, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=79 modulo m=119, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(1)$ and $Y\\sim\\mathrm{Exp}(4)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{4-1}(e^{-1s}-e^{-4s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}-1&-1\\\\-1&3\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (-1)x^2 + (-3)x + (3)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=1$ is a root. Perform synthetic division to factor $(x-1)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (-3)x^2 + (5)x + (-15)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=3$ is a root. Perform synthetic division to factor $(x-3)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 4-unit and 7-unit coins to make exactly 66 units. Find all nonnegative integer solutions (x,y) to 4x+7y=66, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 4x+7y=66 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+7t$, $y=y_0-4t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^8 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=8/4$ with $n=8$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=8/4$ and $n=8$ to evaluate numerically: $S_n\\approx 1793.000000$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 311/795 conversions and variant B has 240/636. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=-0.0138$; SE=0.0259. 95% CI=(-0.0645,0.0369). Conclusion: B is not significantly different at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=40 modulo m=85, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^4$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[-4, 3, 1, -3]$ and $v=[-4, 1, 3, 1]$ one has $\\langle u,v\\rangle=19$, $\\|u\\|=5.9161$, $\\|v\\|=5.1962$, and indeed $|19|\\le 30.7409$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^11 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=5/2$ with $n=11$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=5/2$ and $n=11$ to evaluate numerically: $S_n\\approx 164244.354492$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{5^2}+\\frac{y^2}{5^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{1x}\\cos(4x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{1x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.05. A test has sensitivity 0.97 and specificity 0.93. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.4217. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9983. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 5, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^10 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=7/3$ with $n=10$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=7/3$ and $n=10$ to evaluate numerically: $S_n\\approx 33187.780572$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(3)$ and $Y\\sim\\mathrm{Exp}(5)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{5-3}(e^{-3s}-e^{-5s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=33, mean1=7.41, sd1=3.22) and (n2=53, mean2=4.1, sd2=1.79). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 5.9334$. Test statistic $t=\\dfrac{7.41-4.1}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx 6.1281$ with df=84. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=53, mean1=6.45, sd1=1.98) and (n2=28, mean2=9.24, sd2=2.92). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 5.4946$. Test statistic $t=\\dfrac{6.45-9.24}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx -5.0946$ with df=79. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^12 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=4/5$ with $n=12$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=4/5$ and $n=12$ to evaluate numerically: $S_n\\approx 19.158844$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=100 modulo m=199, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{1x}\\cos(4x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{1x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (3)x^2 + (-3)x + (-5)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=-1$ is a root. Perform synthetic division to factor $(x--1)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=42, mean1=3.72, sd1=2.76) and (n2=33, mean2=5.94, sd2=3.31). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 9.0811$. Test statistic $t=\\dfrac{3.72-5.94}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx -3.1669$ with df=73. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (-3)x^2 + (-4)x + (0)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=4$ is a root. Perform synthetic division to factor $(x-4)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^5 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=4/5$ with $n=5$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=4/5$ and $n=5$ to evaluate numerically: $S_n\\approx 8.616000$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (-1)x^2 + (-5)x + (2)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=-2$ is a root. Perform synthetic division to factor $(x--2)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}-1&3\\\\1&4\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^3$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[3, 1, -1]$ and $v=[3, 0, 3]$ one has $\\langle u,v\\rangle=6$, $\\|u\\|=3.3166$, $\\|v\\|=4.2426$, and indeed $|6|\\le 14.0712$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (-2)x^2 + (-1)x + (2)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=1$ is a root. Perform synthetic division to factor $(x-1)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=38, mean1=9.08, sd1=1.7) and (n2=43, mean2=0.76, sd2=3.16). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 6.6623$. Test statistic $t=\\dfrac{9.08-0.76}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx 14.4775$ with df=79. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^11 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=2/9$ with $n=11$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=2/9$ and $n=11$ to evaluate numerically: $S_n\\approx 1.653060$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 105/982 conversions and variant B has 392/961. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=0.3010$; SE=0.0187. 95% CI=(0.2644,0.3376). Conclusion: B is significantly better at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}1&4\\\\-4&-2\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 5-unit and 9-unit coins to make exactly 101 units. Find all nonnegative integer solutions (x,y) to 5x+9y=101, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 5x+9y=101 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+9t$, $y=y_0-5t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 207/451 conversions and variant B has 73/477. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=-0.3059$; SE=0.0287. 95% CI=(-0.3621,-0.2497). Conclusion: B is significantly worse at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}4&1\\\\-2&3\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}-2&1\\\\-1&0\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{3^2}+\\frac{y^2}{4^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.05. A test has sensitivity 0.97 and specificity 0.93. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.4217. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9983. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^6 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=4/8$ with $n=6$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=4/8$ and $n=6$ to evaluate numerically: $S_n\\approx 3.750000$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=101 modulo m=110, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 4, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{3^2}+\\frac{y^2}{4^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=28, mean1=6.33, sd1=3.61) and (n2=44, mean2=4.61, sd2=3.45). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 12.3382$. Test statistic $t=\\dfrac{6.33-4.61}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx 2.0255$ with df=70. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 5-unit and 9-unit coins to make exactly 110 units. Find all nonnegative integer solutions (x,y) to 5x+9y=110, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 5x+9y=110 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+9t$, $y=y_0-5t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}4&-2\\\\-1&-1\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^10 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=7/2$ with $n=10$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=7/2$ and $n=10$ to evaluate numerically: $S_n\\approx 1059282.343750$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (-2)x^2 + (2)x + (-15)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=3$ is a root. Perform synthetic division to factor $(x-3)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 7, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{3^2}+\\frac{y^2}{4^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=54, mean1=6.61, sd1=3.9) and (n2=41, mean2=5.28, sd2=1.82). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 10.0928$. Test statistic $t=\\dfrac{6.61-5.28}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx 2.0210$ with df=93. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=26, mean1=7.6, sd1=1.94) and (n2=44, mean2=3.86, sd2=2.0). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 3.9131$. Test statistic $t=\\dfrac{7.6-3.86}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx 7.6432$ with df=68. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^8 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=7/9$ with $n=8$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=7/9$ and $n=8$ to evaluate numerically: $S_n\\approx 12.717021$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^6 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=7/6$ with $n=6$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=7/6$ and $n=6$ to evaluate numerically: $S_n\\approx 36.000000$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{3x}\\cos(3x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{3x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^6$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[4, 4, -4, 0, 3, -1]$ and $v=[4, 3, -2, -2, 4, 3]$ one has $\\langle u,v\\rangle=45$, $\\|u\\|=7.6158$, $\\|v\\|=7.6158$, and indeed $|45|\\le 58.0000$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{1x}\\cos(4x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{1x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (2)x^2 + (6)x + (5)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=-1$ is a root. Perform synthetic division to factor $(x--1)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 4-unit and 7-unit coins to make exactly 80 units. Find all nonnegative integer solutions (x,y) to 4x+7y=80, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 4x+7y=80 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+7t$, $y=y_0-4t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[5]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^5-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[2]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^2-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.05. A test has sensitivity 0.92 and specificity 0.96. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.5476. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9956. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^4$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[3, -1, 2, -1]$ and $v=[4, 2, 2, 4]$ one has $\\langle u,v\\rangle=10$, $\\|u\\|=3.8730$, $\\|v\\|=6.3246$, and indeed $|10|\\le 24.4949$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 5-unit and 9-unit coins to make exactly 119 units. Find all nonnegative integer solutions (x,y) to 5x+9y=119, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 5x+9y=119 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+9t$, $y=y_0-5t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.02. A test has sensitivity 0.95 and specificity 0.90. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.1624. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9989. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}-4&-4\\\\-3&4\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=35, mean1=2.41, sd1=2.1) and (n2=38, mean2=2.6, sd2=1.38). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 3.1043$. Test statistic $t=\\dfrac{2.41-2.6}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx -0.4603$ with df=71. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=62 modulo m=183, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{2x}\\cos(4x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{2x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^8 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=8/4$ with $n=8$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=8/4$ and $n=8$ to evaluate numerically: $S_n\\approx 1793.000000$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{2^2}+\\frac{y^2}{4^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 4-unit and 7-unit coins to make exactly 70 units. Find all nonnegative integer solutions (x,y) to 4x+7y=70, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 4x+7y=70 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+7t$, $y=y_0-4t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.01. A test has sensitivity 0.97 and specificity 0.96. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.1968. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9997. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^6$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[-2, 2, -2, 3, -2, -4]$ and $v=[-2, -3, 1, -4, -3, -2]$ one has $\\langle u,v\\rangle=-2$, $\\|u\\|=6.4031$, $\\|v\\|=6.5574$, and indeed $|-2|\\le 41.9881$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (6)x^2 + (4)x + (-8)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=-2$ is a root. Perform synthetic division to factor $(x--2)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{4^2}+\\frac{y^2}{5^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(5)$ and $Y\\sim\\mathrm{Exp}(2)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{2-5}(e^{-5s}-e^{-2s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (-2)x^2 + (-14)x + (3)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=-3$ is a root. Perform synthetic division to factor $(x--3)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{2x}\\cos(5x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{2x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{3x}\\cos(3x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{3x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 4, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^6 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=2/8$ with $n=6$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=2/8$ and $n=6$ to evaluate numerically: $S_n\\approx 1.775391$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(3)$ and $Y\\sim\\mathrm{Exp}(2)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{2-3}(e^{-3s}-e^{-2s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 410/861 conversions and variant B has 48/212. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=-0.2498$; SE=0.0334. 95% CI=(-0.3152,-0.1843). Conclusion: B is significantly worse at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^7$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[-4, 3, 4, -2, 1, 1, -1]$ and $v=[-4, 3, 4, 0, -4, 1, 2]$ one has $\\langle u,v\\rangle=36$, $\\|u\\|=6.9282$, $\\|v\\|=7.8740$, and indeed $|36|\\le 54.5527$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=43 modulo m=75, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^6 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=3/5$ with $n=6$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=3/5$ and $n=6$ to evaluate numerically: $S_n\\approx 5.258560$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(1)$ and $Y\\sim\\mathrm{Exp}(3)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{3-1}(e^{-1s}-e^{-3s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (1)x^2 + (-8)x + (-6)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=-3$ is a root. Perform synthetic division to factor $(x--3)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=22 modulo m=96, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=61 modulo m=94, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}3&4\\\\-3&-2\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 5, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{5^2}+\\frac{y^2}{6^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(1)$ and $Y\\sim\\mathrm{Exp}(2)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{2-1}(e^{-1s}-e^{-2s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^4$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[-1, -3, 2, -4]$ and $v=[-4, 2, 1, 4]$ one has $\\langle u,v\\rangle=-16$, $\\|u\\|=5.4772$, $\\|v\\|=6.0828$, and indeed $|-16|\\le 33.3167$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 78/229 conversions and variant B has 54/357. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=-0.1894$; SE=0.0366. 95% CI=(-0.2611,-0.1176). Conclusion: B is significantly worse at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 3-unit and 5-unit coins to make exactly 66 units. Find all nonnegative integer solutions (x,y) to 3x+5y=66, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 3x+5y=66 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+5t$, $y=y_0-3t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}-3&-4\\\\-3&-3\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "In an A/B experiment, variant A has 340/945 conversions and variant B has 97/912. Construct a 95% CI for $p_B-p_A$ using the normal approximation and state whether B is significantly better.",
    "text_solution": "Point estimate $\\hat p_B-\\hat p_A=-0.2534$; SE=0.0187. 95% CI=(-0.2900,-0.2169). Conclusion: B is significantly worse at 5%.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} CI for difference in proportions (A/B test).\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\hat p_A=\\frac{c_A}{n_A},\\quad \\hat p_B=\\frac{c_B}{n_B},\\quad\n\\mathrm{SE}=\\sqrt{\\frac{\\hat p_A(1-\\hat p_A)}{n_A}+\\frac{\\hat p_B(1-\\hat p_B)}{n_B}}.\n\\]\nA 95\\% CI is $(\\hat p_B-\\hat p_A)\\pm 1.96\\,\\mathrm{SE}$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^7$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[1, -1, 3, 4, 0, 1, -4]$ and $v=[-2, -3, -4, 2, 3, -2, 3]$ one has $\\langle u,v\\rangle=-17$, $\\|u\\|=6.6332$, $\\|v\\|=7.4162$, and indeed $|-17|\\le 49.1935$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{2x}\\cos(4x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{2x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=101 modulo m=109, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(5)$ and $Y\\sim\\mathrm{Exp}(1)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{1-5}(e^{-5s}-e^{-1s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{4^2}+\\frac{y^2}{3^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(3)$ and $Y\\sim\\mathrm{Exp}(4)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{4-3}(e^{-3s}-e^{-4s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 4, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Evaluate $\\displaystyle \\int x e^{3x}\\cos(3x)\\,dx$ using integration by parts twice and organize terms to solve for the integral.",
    "text_solution": "Let $I=\\int x e^{ax}\\cos(bx)\\,dx$. Integrate by parts with $u=x$, $dv=e^{ax}\\cos(bx)dx$. Compute $v=\\frac{e^{ax}}{a^2+b^2}(a\\cos bx+ b\\sin bx)$. Then $I=xv-\\int v\\,dx$. The remaining integral involves $\\int e^{ax}\\cos bx\\,dx$ and $\\int e^{ax}\\sin bx\\,dx$, which satisfy a 2×2 linear system. Solving yields the closed form:\n$I=\\frac{e^{3x}}{(a^2+b^2)^2}\\big((a^2-b^2)x\\cos bx + 2abx\\sin bx + a\\cos bx + b\\sin bx\\big)+C$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Compute $\\int x e^{ax}\\cos(bx)\\,dx$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $I=\\int x e^{ax}\\cos(bx)\\,dx$. With $u=x$, $dv=e^{ax}\\cos(bx)\\,dx$, we have\n\\[\nv=\\frac{e^{ax}}{a^2+b^2}\\bigl(a\\cos(bx)+b\\sin(bx)\\bigr).\n\\]\nThus $I=xv-\\int v\\,dx$. Reduce to $\\int e^{ax}\\cos(bx)\\,dx$ and $\\int e^{ax}\\sin(bx)\\,dx$, solve the resulting linear system, and obtain\n\\[\nI=\\frac{e^{ax}}{(a^2+b^2)^2}\\Big((a^2-b^2)x\\cos(bx)+2ab\\,x\\sin(bx)+a\\cos(bx)+b\\sin(bx)\\Big)+C.\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Design the Extended Euclidean Algorithm to compute integers (x,y) with ax+my=gcd(a,m) and use it to find the modular inverse of a=50 modulo m=62, if it exists. Provide rigorous pseudocode and complexity analysis.",
    "text_solution": "Maintain invariants $old\\_r = a\\cdot old\\_s + m\\cdot old\\_t$ and $r = a\\cdot s + m\\cdot t$. Loop preserves these via division with remainder. On termination, $r=0$ and $old\\_r=\\gcd(a,m)$, with coefficients $(old\\_s,old\\_t)$. If $\\gcd(a,m)=1$, the modular inverse is $a^{-1}\\equiv old\\_s\\pmod m$. The algorithm runs in $O(\\log m)$ arithmetic steps.\n\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Extended Euclidean Algorithm, correctness, and modular inverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaintain\n\\[\n\\begin{aligned}\n\\text{old\\_r} &= a\\,\\text{old\\_s}+m\\,\\text{old\\_t},\\\\\nr &= a\\,s+m\\,t.\n\\end{aligned}\n\\]\nDivision with remainder preserves these invariants. When $r=0$ we have $\\gcd(a,m)=\\text{old\\_r}$ and Bézout coefficients $(\\text{old\\_s},\\text{old\\_t})$.\nIf $\\gcd(a,m)=1$, then $a^{-1}\\equiv \\text{old\\_s}\\pmod m$. The loop executes $O(\\log m)$ iterations.\n\\begin{verbatim}\nExtendedGCD(a, m):\n  (old_r, r) = (a, m)\n  (old_s, s) = (1, 0)\n  (old_t, t) = (0, 1)\n  while r != 0:\n    q = old_r // r\n    (old_r, r) = (r, old_r - q*r)\n    (old_s, s) = (s, old_s - q*s)\n    (old_t, t) = (t, old_t - q*t)\n  return (old_r, old_s, old_t)  # gcd, x, y\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Given $A=\\begin{pmatrix}-4&3\\\\-4&-4\\end{pmatrix}$ with distinct eigenvalues, diagonalize $A$ and compute $e^{At}$ in closed form.",
    "text_solution": "Solve $\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0$ for eigenvalues. For each eigenvalue $\\lambda$, find an eigenvector and form $A=V\\Lambda V^{-1}$. Then $e^{At}=V e^{\\Lambda t} V^{-1}$ with $e^{\\Lambda t}=\\operatorname{diag}(e^{\\lambda_1 t},e^{\\lambda_2 t})$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Diagonalization and matrix exponential for $2\\times 2$.\n\n\\bigskip\n\\textbf{Solution.}\n\nFind eigenvalues from\n\\[\n\\lambda^2-\\operatorname{tr}(A)\\lambda+\\det(A)=0.\n\\]\nWith $A=V\\Lambda V^{-1}$, we have\n\\[\ne^{At}=V e^{\\Lambda t} V^{-1}=\\; V\\begin{pmatrix}e^{\\lambda_1 t}&0\\\\0&e^{\\lambda_2 t}\\end{pmatrix}V^{-1}.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Find the rectangle of maximum area that can be inscribed in the ellipse $\\frac{x^2}{2^2}+\\frac{y^2}{3^2}=1$ with sides parallel to the axes. Use Lagrange multipliers and provide the maximal area.",
    "text_solution": "Maximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2 + y^2/B^2 -1=0$. Solve $\\nabla A=\\lambda\\nabla g$: $4y=\\lambda\\,2x/A^2$, $4x=\\lambda\\,2y/B^2$, together with the constraint. Eliminating $\\lambda$ gives $y/x = B^2/A^2\\cdot x/y$, so $x^2/A^2=y^2/B^2$. Using the constraint, $2x^2/A^2=1$ so $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. Max area $=4xy=2AB$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Max-area rectangle in an ellipse via Lagrange multipliers.\n\n\\bigskip\n\\textbf{Solution.}\n\nMaximize $A(x,y)=4xy$ subject to $g(x,y)=x^2/A^2+y^2/B^2-1=0$.\n\\[\n\\nabla A=(4y,4x)=\\lambda \\left(\\frac{2x}{A^2},\\frac{2y}{B^2}\\right).\n\\]\nHence $\\dfrac{y}{x}=\\dfrac{\\lambda}{2}\\dfrac{1}{A^2}$ and $\\dfrac{x}{y}=\\dfrac{\\lambda}{2}\\dfrac{1}{B^2}$, so $x^2/A^2=y^2/B^2$.\nWith the constraint this gives $x=A/\\sqrt{2}$ and $y=B/\\sqrt{2}$. The maximal area is $4xy=2AB$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "A cashier uses only 5-unit and 9-unit coins to make exactly 74 units. Find all nonnegative integer solutions (x,y) to 5x+9y=74, and identify the solution with the fewest coins.",
    "text_solution": "Find one solution to 5x+9y=74 using the Extended Euclidean Algorithm. General solutions have the form $x=x_0+9t$, $y=y_0-5t$ for integer $t$. Restrict to $x,y\\ge 0$ to get an interval of $t$. Among feasible $t$, minimize $x+y$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Coin problem via extended Euclid and parametric solutions.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve $ax+by=T$ by first finding one solution $(x_0,y_0)$ using the extended Euclidean algorithm.\nAll solutions are\n\\[\nx=x_0+\\frac{T}{\\gcd(a,b)}\\cdot \\frac{b}{\\gcd(a,b)}\\,t,\\qquad\ny=y_0-\\frac{T}{\\gcd(a,b)}\\cdot \\frac{a}{\\gcd(a,b)}\\,t,\\quad t\\in\\mathbb{Z}.\n\\]\nRestrict to $x,y\\ge 0$ to obtain feasible $t$ and minimize $x+y$ over these.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(4)$ and $Y\\sim\\mathrm{Exp}(3)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{3-4}(e^{-4s}-e^{-3s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[7]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^7-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "A disease has prevalence 0.10. A test has sensitivity 0.95 and specificity 0.96. Compute PPV and NPV, and discuss how they change with prevalence.",
    "text_solution": "PPV = P(D|+) = (sens·prev)/(sens·prev + (1-spec)(1-prev)) = 0.7252. NPV = P(~D|-) = (spec·(1-prev))/((1-sens)·prev + spec·(1-prev)) = 0.9942. Higher prevalence increases PPV and decreases NPV; lower prevalence has the opposite effect.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Bayesian predictive values.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\mathrm{PPV}=\\frac{\\mathrm{sens}\\cdot \\mathrm{prev}}{\\mathrm{sens}\\cdot \\mathrm{prev}+(1-\\mathrm{spec})(1-\\mathrm{prev})},\\qquad\n\\mathrm{NPV}=\\frac{\\mathrm{spec}\\cdot (1-\\mathrm{prev})}{(1-\\mathrm{sens})\\cdot \\mathrm{prev}+\\mathrm{spec}\\cdot (1-\\mathrm{prev})}.\n\\]\nLarger prevalence raises PPV and lowers NPV.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^3$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[-3, -3, 4]$ and $v=[1, -3, -3]$ one has $\\langle u,v\\rangle=-6$, $\\|u\\|=5.8310$, $\\|v\\|=4.3589$, and indeed $|-6|\\le 25.4165$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "A Markov chain has transition matrix $P$ (3 states). Compute the stationary distribution $\\pi$ solving $\\pi^T P=\\pi^T$, $\\sum\\pi_i=1$, and interpret it.",
    "text_solution": "Solve the linear system $(P^T-I)\\pi=0$ with $\\sum\\pi_i=1$. The stationary distribution gives the long-run fraction of time spent in each state, assuming irreducibility and aperiodicity.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Stationary distribution of a 3-state Markov chain.\n\n\\bigskip\n\\textbf{Solution.}\n\nSolve\n\\[\n\\pi^T P=\\pi^T,\\qquad \\sum_{i=1}^3 \\pi_i=1,\n\\]\nequivalently $(P^T-I)\\pi=0$ with the normalization constraint. Interpret $\\pi$ as the long-run proportions under standard ergodicity assumptions.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "word_problem",
    "difficulty": "hard"
  },
  {
    "problem": "Let $X\\sim\\mathrm{Exp}(1)$ and $Y\\sim\\mathrm{Exp}(2)$ independent with distinct rates. Derive the pdf and cdf of $S=X+Y$.",
    "text_solution": "The convolution gives $f_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\, \\lambda_2 e^{-\\lambda_2 (s-x)}\\,dx = \\frac{\\lambda_1\\lambda_2}{2-1}(e^{-1s}-e^{-2s})$ for $s\\ge 0$. Integrate to obtain $F_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Hypoexponential (sum of two exponentials with distinct rates).\n\n\\bigskip\n\\textbf{Solution.}\n\nFor $s\\ge 0$,\n\\[\nf_S(s)=\\int_0^s \\lambda_1 e^{-\\lambda_1 x}\\,\\lambda_2 e^{-\\lambda_2(s-x)}\\,dx\n= \\frac{\\lambda_1\\lambda_2}{\\lambda_2-\\lambda_1}\\big(e^{-\\lambda_1 s}-e^{-\\lambda_2 s}\\big).\n\\]\nIntegrating,\n\\[\nF_S(s)=1-\\frac{\\lambda_2 e^{-\\lambda_1 s}-\\lambda_1 e^{-\\lambda_2 s}}{\\lambda_2-\\lambda_1}.\n\\]\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^7 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=2/4$ with $n=7$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=2/4$ and $n=7$ to evaluate numerically: $S_n\\approx 3.859375$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Let $A\\in\\mathbb{R}^{n\\times n}$ be diagonalizable with distinct eigenvalues. Prove that eigenvectors corresponding to distinct eigenvalues are linearly independent, and conclude that $A$ has $n$ independent eigenvectors.",
    "text_solution": "Suppose $Av_i=\\lambda_i v_i$ with $\\lambda_i\\ne\\lambda_j$ for $i\\ne j$. If $\\sum c_i v_i=0$, apply $A$ to get $\\sum c_i \\lambda_i v_i=0$. Subtract $\\lambda_1\\sum c_i v_i=0$ to obtain $\\sum c_i (\\lambda_i-\\lambda_1) v_i=0$. Inductively, each $c_i=0$. Therefore, eigenvectors for distinct eigenvalues are independent; with $n$ distinct eigenvalues we obtain $n$ independent eigenvectors.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Independence of eigenvectors for distinct eigenvalues.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Av_i=\\lambda_i v_i$, $\\lambda_i\\neq \\lambda_j$ for $i\\neq j$. Assume $\\sum_{i=1}^k c_i v_i=0$ with some $c_i\\not=0$.\nApplying $A$ and forming linear combinations shows recursively that each $c_i=0$. Thus the set is independent.\nIf $A$ has $n$ distinct eigenvalues, it has $n$ linearly independent eigenvectors.\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Solve the cubic equation $x^3 + (0)x^2 + (-8)x + (8)=0$ by rational root test and factorization.",
    "text_solution": "By the Rational Root Theorem, any rational root must divide the constant term. Try integer divisors to find one root $r$. Here, testing shows $x=2$ is a root. Perform synthetic division to factor $(x-2)$ and obtain a quadratic factor, then solve it via the quadratic formula.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Solve a cubic via rational root test and factorization.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Rational Root Theorem, rational roots divide the constant term. Once a root $r$ is found, factor the cubic as $(x-r)q(x)$.\nUse synthetic division to compute $q(x)$ and then solve $q(x)=0$ via the quadratic formula.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the Taylor series of $\\arctan x$ about $x=0$ up to order 4, include the Lagrange remainder, and determine the interval of convergence.",
    "text_solution": "Since $(\\arctan x)'=1/(1+x^2)$, expand $(1+x^2)^{-1}=\\sum_{k\\ge0}(-1)^k x^{2k}$ for $|x|<1$ and integrate termwise: $\\arctan x=\\sum_{k\\ge0}(-1)^k x^{2k+1}/(2k+1)$ for $|x|<1$. At $x=\\pm1$, the series alternates and converges at $x=1$ to $\\pi/4$ and at $x=-1$ to $-\\pi/4$. The Lagrange remainder after $N$ terms is $R_N(x)=\\dfrac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1}$ for some $\\xi$ between $0$ and $x$; one can bound $|f^{(N+1)}(\\xi)|$ using derivatives of $1/(1+x^2)$ on $[-1,1]$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Series and remainder for $\\arctan x$.\n\n\\bigskip\n\\textbf{Solution.}\n\nSince $\\dfrac{d}{dx}\\arctan x=\\dfrac{1}{1+x^2}$,\n\\[\n\\frac{1}{1+x^2}=\\sum_{k=0}^{\\infty}(-1)^k x^{2k},\\quad |x|<1.\n\\]\nIntegrating termwise gives\n\\[\n\\arctan x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{2k+1}\\,x^{2k+1},\\quad |x|<1.\n\\]\nThe series converges at $x=\\pm 1$ (alternating). The Lagrange remainder after $N$ terms is\n\\[\nR_N(x)=\\frac{f^{(N+1)}(\\xi)}{(N+1)!}x^{N+1},\\quad \\xi\\in(0,x).\n\\]\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove the Cauchy–Schwarz inequality in $\\mathbb{R}^5$ and characterize equality. Then verify it for two explicit vectors.",
    "text_solution": "Consider $p(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge0$ for all $t\\in\\mathbb{R}$. A quadratic $at^2+bt+c\\ge 0$ for all $t$ implies $b^2\\le 4ac$. Thus $4\\langle u,v\\rangle^2\\le 4\\|u\\|^2\\|v\\|^2$, giving $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ with $tu=v$ (or $v=0$ or $u=0$), i.e., iff $u$ and $v$ are linearly dependent.\n\nFor $u=[1, -1, 2, -4, -3]$ and $v=[4, -3, 1, -3, -1]$ one has $\\langle u,v\\rangle=24$, $\\|u\\|=5.5678$, $\\|v\\|=6.0000$, and indeed $|24|\\le 33.4066$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Prove Cauchy–Schwarz and identify the equality case; then check a numerical example.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $u,v\\in\\mathbb{R}^n$. Consider\n\\[\np(t)=\\|tu-v\\|^2=\\langle tu-v,tu-v\\rangle=t^2\\|u\\|^2-2t\\langle u,v\\rangle+\\|v\\|^2\\ge 0,\\ \\forall t\\in\\mathbb{R}.\n\\]\nHence the discriminant satisfies\n\\[\n(-2\\langle u,v\\rangle)^2\\le 4\\|u\\|^2\\|v\\|^2,\n\\]\nwhich implies $|\\langle u,v\\rangle|\\le \\|u\\|\\,\\|v\\|$. Equality holds iff there exists $t$ such that $tu=v$, i.e., $u$ and $v$ are linearly dependent.\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "For i.i.d. $X_1,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)$ with both parameters unknown, derive the MLEs $\\hat\\mu$ and $\\hat\\sigma^2$ and show that $\\hat\\sigma^2$ is biased.",
    "text_solution": "Log-likelihood $\\ell(\\mu,\\sigma^2)=-\\tfrac{n}{2}\\log(2\\pi\\sigma^2) - \\tfrac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2$. Setting derivatives to zero gives $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\tfrac{1}{n}\\sum (X_i-\\bar X)^2$. Then $\\mathbb{E}[\\hat\\sigma^2]=\\tfrac{n-1}{n}\\sigma^2$ shows bias.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MLEs for normal with unknown mean and variance.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\ell(\\mu,\\sigma^2)=-\\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum (X_i-\\mu)^2.\n\\]\nFOC yield $\\hat\\mu=\\bar X$ and $\\hat\\sigma^2=\\frac{1}{n}\\sum (X_i-\\bar X)^2$, with\n\\[\n\\mathbb{E}[\\hat\\sigma^2]=\\frac{n-1}{n}\\sigma^2.\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Two independent normal samples: (n1=40, mean1=8.85, sd1=2.85) and (n2=52, mean2=3.66, sd2=3.58). Test $H_0:\\mu_1=\\mu_2$ vs $H_1:\\mu_1\\ne\\mu_2$ using a pooled-variance t-test at $\\alpha=0.05$. Provide the test statistic, df, and decision.",
    "text_solution": "Pooled variance $s_p^2 = 10.7824$. Test statistic $t=\\dfrac{8.85-3.66}{\\sqrt{s_p^2(1/n_1+1/n_2)}}\\approx 7.5153$ with df=90. Reject for $|t|>t_{\\alpha/2,df}$. Compare to the critical value or compute a p-value.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Two-sample pooled t-test from summary stats.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\ns_p^2=\\frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2},\\qquad\nt=\\frac{\\bar X_1-\\bar X_2}{\\sqrt{s_p^2\\left(\\frac1{n_1}+\\frac1{n_2}\\right)}},\\quad \\text{df}=n_1+n_2-2.\n\\]\nReject $H_0$ when $|t|>t_{\\alpha/2,\\text{df}}$ (or small p-value).\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "computation",
    "difficulty": "hard"
  },
  {
    "problem": "Outline a numerically stable QR-based algorithm to solve overdetermined least squares $\\min_x\\|Ax-b\\|_2$ with $A\\in\\mathbb{R}^{m\\times n}$, $m\\ge n$. Provide Householder QR pseudocode, complexity, and conditioning remarks.",
    "text_solution": "Form $A=QR$ with Householder reflections; then $\\|Ax-b\\|_2=\\|Rx-Q^Tb\\|_2$, and solve the upper-triangular system for $x$. Householder QR is $O(mn^2)$ and more stable than normal equations. Conditioning is governed by $\\kappa(A)$; QR avoids squaring the condition number as in $A^TA$.\n\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least squares via Householder QR.\n\n\\bigskip\n\\textbf{Solution.}\n\nCompute $A=QR$ with $Q$ orthogonal and $R$ upper-triangular. Then minimize $\\|Rx-Q^Tb\\|_2$ and solve $Rx=y_1$.\nHouseholder QR costs $O(mn^2)$ and is numerically stable.\n\\begin{verbatim}\nHouseholderQR(A):\n  for k in 1..n:\n    v = reflector(A[k:m, k])\n    A[k:m, k:n] -= 2 v (v^T A[k:m, k:n])\n    R[k,k] = ... ; Q accumulates reflectors implicitly\n  return (Q,R)\nSolveLS(A,b):\n  (Q,R) = HouseholderQR(A)\n  y = Q^T b\n  solve R x = y[1:n]\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Design a robust Newton method to compute $\\sqrt[6]{\\alpha}$ for $\\alpha>0$ by solving $f(x)=x^6-\\alpha=0$. Provide pseudocode with safeguards (step damping and derivative checks), stopping criteria, and a brief convergence discussion.",
    "text_solution": "Use $x_{k+1}=x_k - f(x_k)/f'(x_k)$ with $f(x)=x^c-\\alpha$. Guard against $f'(x)=0$ and negative iterates; apply step damping to keep $x_k>0$. Stop when the relative change is below $\\text{tol}$ or $|f(x_k)|$ is small. Locally quadratic convergence holds when starting sufficiently close to the root.\n\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Robust Newton for $x^c=\\alpha$.\n\n\\bigskip\n\\textbf{Solution.}\n\nWe solve $f(x)=x^c-\\alpha=0$ with Newton:\n\\[\nx_{k+1}=x_k-\\frac{f(x_k)}{f'(x_k)}=x_k-\\frac{x_k^c-\\alpha}{c\\,x_k^{c-1}}.\n\\]\nSafeguards (damping, positivity enforcement) and stopping rules (relative change or residual) improve robustness.\n\\begin{verbatim}\nNewtonRoot(alpha, c, x0, tol, maxit):\n  for k in 1..maxit:\n    f = x0**c - alpha\n    fp = c * x0**(c-1)\n    if fp == 0: return FAIL\n    step = f/fp\n    # damping\n    x1 = x0 - step\n    if x1 <= 0: x1 = 0.5*(x0)\n    if abs(x1-x0) < tol*(1+abs(x1)): return x1\n    x0 = x1\n  return x0\n\\end{verbatim}\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Derive the normal equations for least squares, prove positive definiteness of $A^TA$ for full column rank, and express the solution using the Moore–Penrose pseudoinverse.",
    "text_solution": "Minimize $\\|Ax-b\\|_2^2=(Ax-b)^T(Ax-b)$; differentiating gives $2A^T(Ax-b)=0$, i.e., $A^TAx=A^Tb$. If $A$ has full column rank, then $A^TA$ is symmetric positive definite because $x^T A^T A x=\\|Ax\\|_2^2>0$ for $x\\ne 0$. The unique minimizer is $x^*=(A^TA)^{-1}A^T b= A^{\\dagger} b$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Least-squares via normal equations and pseudoinverse.\n\n\\bigskip\n\\textbf{Solution.}\n\nMinimizing $\\|Ax-b\\|_2^2$ yields\n\\[\nA^TAx=A^Tb.\n\\]\nIf $A$ has full column rank,\n\\[\nx^T A^T A x=\\|Ax\\|_2^2>0\\quad(x\\neq 0),\n\\]\nso $A^TA$ is positive definite and invertible. Then\n\\[\nx^*=(A^TA)^{-1}A^Tb=A^\\dagger b.\n\\]\n\n\\end{document}\n",
    "topic": "linear_algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the alias method for sampling from a discrete distribution on $n$ outcomes in $O(1)$ time after $O(n)$ preprocessing. Provide pseudocode for the preprocess and sample steps, and analyze memory usage.",
    "text_solution": "The alias method represents each outcome by a column with adjusted probability and an alias. Sampling uses one random integer and one uniform. Preprocessing and memory are $O(n)$; sampling is $O(1)$.\n\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Alias method for $O(1)$ sampling.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\begin{verbatim}\nAliasPreprocess(p[1..n]):\n  scale = n * p\n  split into Small and Large indices\n  while Small and Large nonempty:\n    i = pop(Small); j = pop(Large)\n    prob[i] = scale[i]; alias[i] = j\n    scale[j] = scale[j] - (1 - prob[i])\n    if scale[j] < 1: push Small; else push Large\n  fill remaining with prob=1\n\nAliasSample(prob, alias):\n  i = uniform_int(1..n)\n  u = uniform(0,1)\n  return i if u < prob[i] else alias[i]\n\\end{verbatim}\nSampling cost is $O(1)$; preprocessing and storage are $O(n)$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Compute the moment generating function (mgf) of a normal random variable $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ and use it to derive $\\mathbb{E}[X]$ and $\\mathrm{Var}(X)$.",
    "text_solution": "Complete the square: $M_X(t)=\\mathbb{E}[e^{tX}]=\\exp(\\mu t + \\tfrac{1}{2}\\sigma^2 t^2)$. Then $M'_X(0)=\\mu$ and $M''_X(0)-M'_X(0)^2=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} MGF of the normal and moments.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\nM_X(t)=\\mathbb{E}[e^{tX}]=\\exp\\!\\left(\\mu t+\\frac{\\sigma^2 t^2}{2}\\right).\n\\]\nThus $M_X'(0)=\\mu$ and $M_X''(0)-M_X'(0)^2=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Describe the EM algorithm for a two-component univariate Gaussian mixture with parameters $(\\pi,\\mu_1,\\mu_2,\\sigma_1^2,\\sigma_2^2)$. Provide explicit E-step and M-step updates and pseudocode; discuss initialization and convergence diagnostics.",
    "text_solution": "Introduce latent indicators $Z_i\\in\\{1,2\\}$. The E-step computes responsibilities $\\gamma_{i1}=\\mathbb{P}(Z_i=1\\mid x_i,\\theta)$; the M-step maximizes the expected complete log-likelihood to update parameters. Initialize via k-means or random splits; monitor the log-likelihood for monotone increase; stop when increments fall below a tolerance.\n\nEM_GMM(x):\n  init (pi, mu1, mu2, s1, s2)\n  repeat until convergence:\n    # E-step\n    gamma1_i = pi * N(x_i | mu1, s1) / [pi * N(x_i | mu1, s1) + (1-pi) * N(x_i | mu2, s2)]\n    gamma2_i = 1 - gamma1_i\n    # M-step\n    N1 = sum(gamma1_i); N2 = sum(gamma2_i)\n    mu1 = sum(gamma1_i * x_i)/N1;  mu2 = sum(gamma2_i * x_i)/N2\n    s1 = sum(gamma1_i * (x_i-mu1)^2)/N1; s2 = sum(gamma2_i * (x_i-mu2)^2)/N2\n    pi = N1 / n\n",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} EM for a 2-component Gaussian mixture.\n\n\\bigskip\n\\textbf{Solution.}\n\nE-step: $\\gamma_{i1}=\\dfrac{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)}{\\pi\\,\\phi(x_i;\\mu_1,\\sigma_1^2)+(1-\\pi)\\,\\phi(x_i;\\mu_2,\\sigma_2^2)}$, $\\gamma_{i2}=1-\\gamma_{i1}$.\nM-step:\n\\[\n\\begin{aligned}\nN_1&=\\sum_i \\gamma_{i1},\\quad N_2=\\sum_i \\gamma_{i2},\\\\\n\\mu_1&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}x_i,\\quad \\mu_2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}x_i,\\\\\n\\sigma_1^2&=\\frac{1}{N_1}\\sum_i \\gamma_{i1}(x_i-\\mu_1)^2,\\quad\n\\sigma_2^2=\\frac{1}{N_2}\\sum_i \\gamma_{i2}(x_i-\\mu_2)^2,\\\\\n\\pi&=\\frac{N_1}{n}.\n\\end{aligned}\n\\]\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "algorithm",
    "difficulty": "hard"
  },
  {
    "problem": "Prove that a differentiable function on a closed interval $[a,b]$ is uniformly continuous, and derive an explicit uniform continuity bound using the Mean Value Theorem.",
    "text_solution": "If $f$ is differentiable on $(a,b)$ and continuous on $[a,b]$, then $|f'(x)|$ attains a finite maximum $M$ by the Extreme Value Theorem. For any $x,y\\in[a,b]$, the Mean Value Theorem gives $|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|$. Hence choose $\\delta=\\epsilon/M$ (for $M>0$; if $M=0$, $f$ is constant). This proves uniform continuity with a Lipschitz bound.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Uniform continuity from differentiability on $[a,b]$.\n\n\\bigskip\n\\textbf{Solution.}\n\nBy the Extreme Value Theorem, $|f'|$ attains a maximum $M$ on $[a,b]$ (possibly $M=0$). By the Mean Value Theorem,\n\\[\n|f(x)-f(y)|=|f'(c)||x-y|\\le M|x-y|.\n\\]\nGiven $\\epsilon>0$, choose $\\delta=\\epsilon/M$ (or any $\\delta>0$ if $M=0$). Then $|x-y|<\\delta$ implies $|f(x)-f(y)|<\\epsilon$.\nThus $f$ is uniformly continuous and Lipschitz with constant $M$.\n\n\\end{document}\n",
    "topic": "calculus",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Show that the unbiased sample variance $S^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar X)^2$ satisfies $\\mathbb{E}[S^2]=\\sigma^2$ for i.i.d. data with variance $\\sigma^2$.",
    "text_solution": "Use the identity $\\sum (X_i-\\bar X)^2=\\sum (X_i-\\mu)^2 - n(\\bar X-\\mu)^2$. Taking expectations yields $\\mathbb{E}[\\sum (X_i-\\bar X)^2]=(n-1)\\sigma^2$. Divide by $n-1$ to get $\\mathbb{E}[S^2]=\\sigma^2$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Unbiasedness of $S^2$.\n\n\\bigskip\n\\textbf{Solution.}\n\n\\[\n\\sum_{i=1}^n (X_i-\\bar X)^2=\\sum_{i=1}^n (X_i-\\mu)^2 - n(\\bar X-\\mu)^2.\n\\]\nTaking expectations gives $(n-1)\\sigma^2$, hence $\\mathbb{E}[S^2]=\\sigma^2$.\n\n\\end{document}\n",
    "topic": "statistics",
    "problem_type": "proof",
    "difficulty": "hard"
  },
  {
    "problem": "Derive a closed form for $S_n=\\sum_{k=1}^9 k r^{k-1}$ for $r\\ne 1$ using differentiation of the geometric series, and evaluate it at $r=2/3$ with $n=9$.",
    "text_solution": "Start with $G_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r}$. Differentiate w.r.t. $r$ to obtain $G_n'(r)=\\sum_{k=1}^n k r^{k-1} = \\frac{1 - (n+1)r^n + n r^{n+1}}{(1-r)^2}$ for $r\\ne 1$. Substitute $r=2/3$ and $n=9$ to evaluate numerically: $S_n\\approx 8.063557$.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Differentiate the finite geometric sum to obtain $\\sum k r^{k-1}$.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet\n\\[\nG_n(r)=\\sum_{k=0}^{n} r^k=\\frac{1-r^{n+1}}{1-r},\\qquad r\\ne 1.\n\\]\nDifferentiating with respect to $r$ gives\n\\[\nG_n'(r)=\\sum_{k=1}^n k r^{k-1}=\\frac{1-(n+1)r^n + n r^{n+1}}{(1-r)^2}.\n\\]\n\n\\end{document}\n",
    "topic": "algebra",
    "problem_type": "derivation",
    "difficulty": "hard"
  },
  {
    "problem": "Prove Chebyshev's inequality from Markov's inequality and discuss when the bound is tight; apply it to bound $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)$.",
    "text_solution": "Apply Markov to $Y=(X-\\mu)^2\\ge 0$: $\\mathbb{P}((X-\\mu)^2\\ge k^2\\sigma^2)\\le \\mathbb{E}[Y]/(k^2\\sigma^2)=1/k^2$. The bound can be tight for two-point distributions symmetric about the mean with appropriate masses.",
    "latex_solution": "\\documentclass{article}\n\\usepackage{amsmath,amssymb,amsthm,mathtools,verbatim}\n\\usepackage[margin=1in]{geometry}\n\\begin{document}\n\\textbf{Problem.} Chebyshev via Markov.\n\n\\bigskip\n\\textbf{Solution.}\n\nLet $Y=(X-\\mu)^2\\ge 0$. By Markov,\n\\[\n\\mathbb{P}\\big((X-\\mu)^2\\ge k^2\\sigma^2\\big)\\le \\frac{\\mathbb{E}[Y]}{k^2\\sigma^2}=\\frac{1}{k^2}.\n\\]\nThus $\\mathbb{P}(|X-\\mu|\\ge k\\sigma)\\le 1/k^2$.\n\n\\end{document}\n",
    "topic": "probability",
    "problem_type": "proof",
    "difficulty": "hard"
  }
]